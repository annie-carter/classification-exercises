{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ce2174",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (prepare.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 13\u001b[0;36m\n\u001b[0;31m    import prepare\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/codeup-data-science/classification-exercises/prepare.py:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    mport pandas as pd\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pydataset import data\n",
    "import seaborn as sns\n",
    "# acquire\n",
    "from env import user, password, hostname, get_db_url\n",
    "from pydataset import data\n",
    "\n",
    "\n",
    "\n",
    "import acquire\n",
    "import prepare\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a766e03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "207a689d",
   "metadata": {},
   "source": [
    "#### Excercise \n",
    "1. Fit the Random Forest classifier to your training sample and transform (i.e. make predictions on the training sample) setting the random_state accordingly and setting min_samples_leaf = 1 and max_depth = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63350e2",
   "metadata": {},
   "source": [
    "2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc4a90",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f0a73",
   "metadata": {},
   "source": [
    "4. Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab5bca",
   "metadata": {},
   "source": [
    "5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e8403",
   "metadata": {},
   "source": [
    "6.After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90177482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data using function from acquire \n",
    "titanic_original = acquire.get_titanic_data()\n",
    "\n",
    "# peek into data\n",
    "titanic_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1473f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data using function from prepare \n",
    "titanic_clean = prepare.prep_titanic(titanic_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ac11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek into clean data\n",
    "titanic_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf46b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data using funciton form prepare module \n",
    "train, validate, test = prepare.train_validate_test_split(titanic_clean,'survived')\n",
    "\n",
    "# get shape of train, validate and test data\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38528722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek into train data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "x_train = train.drop(columns = ['survived'])\n",
    "y_train = train['survived']\n",
    "\n",
    "x_validate =validate.drop(columns = ['survived'])\n",
    "y_validate = validate['survived']\n",
    "\n",
    "x_test = test.drop(columns = ['survived'])\n",
    "y_test = test['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model1\n",
    "seed = 40\n",
    "\n",
    "rf = RandomForestClassifier(min_samples_leaf =1,max_depth=10, random_state=42, max_samples=0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0043ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e654304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predictions \n",
    "train_predictions = rf.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val predictions \n",
    "val_predictions = rf.predict(x_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a80d07",
   "metadata": {},
   "source": [
    "#### 2. Evaluate your results using the model score, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03274ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy( model accuracy)\n",
    "train_accuracy = rf.score(x_train, y_train)\n",
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val accuracy\n",
    "validate_accuracy = rf.score(x_validate, y_validate)\n",
    "validate_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (y_train_predictions, y_train)\n",
    "cm = confusion_matrix(y_train, train_predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report from train predictions\n",
    "print(classification_report(y_train, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab36816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report from validate predictions\n",
    "print(classification_report(y_validate, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bdcc78",
   "metadata": {},
   "source": [
    "#### 3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate truenegative, falsepositive, falsenegative,truepositive \n",
    "TN, FP, FN, TP = cm.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53668280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "ALL = TP + FP + FN + TN\n",
    "acc = (TP + TN) / ALL\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positive rate or precision\n",
    "precision = TPR = TP / (TP + FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positive rate\n",
    "FPR = FP / (FP + TN)\n",
    "FPR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true neagative rate\n",
    "TNR = TN / (TN + FP)\n",
    "TNR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cebcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false negative rate\n",
    "FNR = FN / (FN + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "recall = TP / (TP + FN)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b57c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score\n",
    "f1_score = 2 * (precision*recall) / (precision+recall)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support\n",
    "support_pos = TP + FN\n",
    "support_neg = FP + TN\n",
    "support_pos, support_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc9ca6",
   "metadata": {},
   "source": [
    "#### 4. Run through steps increasing your min_samples_leaf and decreasing your max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fccd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model2\n",
    "seed =40\n",
    "\n",
    "rf2 = RandomForestClassifier(min_samples_leaf =5,max_depth=5, random_state=42, max_samples=0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the random forest algorithm to the training data\n",
    "rf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train predictions from random forest algorithm\n",
    "train_predictions2 = rf2.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df725e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val predictions from random forest algorithm\n",
    "val_predictions2 = rf2.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10374cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy (model accuracy)\n",
    "train_accuracy2 = rf2.score(x_train, y_train)\n",
    "train_accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val accuracy\n",
    "val_accuracy2 = rf2.score(x_validate, y_validate)\n",
    "val_accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (y_train_pred, y_train)\n",
    "cm = confusion_matrix(y_train, train_predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report on train\n",
    "print(classification_report(y_train, train_predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfeb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report from val predictions\n",
    "print(classification_report(y_validate, val_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f434389",
   "metadata": {},
   "source": [
    "#### 5. What are the differences in the evaluation metrics? Which performs better on your in-sample data? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy (model accuracy) on train\n",
    "train_accuracy, train_accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f2823",
   "metadata": {},
   "source": [
    "Model 1 perfroms better because it has higher accuracy score than model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1978f",
   "metadata": {},
   "source": [
    "#### 6. After making a few models, which one has the best performance (or closest metrics) on both train and validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd55cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy (model accuracy) on validate\n",
    "validate_accuracy, val_accuracy2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
